sd <-6250
erro <-500
nc<-(1-0.95)/2
n <-((qnorm(nc,lower.tail= F)*sd)/erro)^2
n"
sd <-4250
erro <-300
nc<-(1-0.9)/2
n <-((qnorm(nc,lower.tail= F)*sd)/erro)^2
n
sd <- 5
N <- 1500
erro <- 1.5
nc <- (1-0.955)/2
n <-(qnorm(nc, lower.tail= F)^2 * sd^2 * N)/((erro^2*(N-1))+(qnorm(nc,lower.tail = F)^2*sd^2))
n
sd <- (30-10)/4
erro <- 1
nc <- (1-0.98)/2
n <-((qnorm(nc,lower.tail= F)*sd)/erro)^2
n
sd <- (30-10)/4
N <- 5000
erro <- 1
nc <- (1-0.98)/2
n <-(qnorm(nc, lower.tail= F)^2 * sd^2 * N)/((erro^2*(N-1))+(qnorm(nc,lower.tail = F)^2*sd^2))
n
pnorm(q=30,mean =  50,sd =  10)
1-pnorm(q=30,mean =  50,sd =  10)
pnorm(q=30,mean =  50,sd =  10)
qnorm(q=30,mean =  50,sd =  10)
rnorm()
rnorm(100)
pnorm(1)
pnorm(1)-pnorm(1, lower.tail = F)
1-pnorm(1)-pnorm(1)
1-pnorm(1)-pnorm(1,lower.tail = F)
pnorm(1)-1+pnorm(1)
pnorm(1)
1-pnorm(-2)
pnorm(1)
1-pnorm(-2)
1-pnorm(1)
pnorm(-2)
pnorm(0)
1-pnorm(1.28)
pnorm(mean = 90, sd = 21)
qnorm(p = 0.90,mean = 90, sd = 21)
qnorm(p = 0.90,mean = 90, sd = 21, lower.tail = F)
#Desviopadrão
d <-3.8
#Médiadaamostra
x <-30.2
#Tamanhodaamostra
n <-100
#Nível de confiança
nc<-(1-0.95)/2
#Erro
error <-d/sqrt(n)
#Limiteinferior
left <-x-(qnorm(nc,lower.tail= F)*error)
#Limitesuperior
right <-x+(qnorm(nc,lower.tail= F)*error)>cat("[",left, "-", right,"]")
#Desviopadrão
d <-3.8
#Médiadaamostra
x <-30.2
#Tamanhodaamostra
n <-100
#Nível de confiança
nc<-(1-0.95)/2
#Erro
error <-d/sqrt(n)
#Limiteinferior
left <-x-(qnorm(nc,lower.tail= F)*error)
#Limitesuperior
right <-x+(qnorm(nc,lower.tail= F)*error)
cat("[",left, "-", right,"]")
#Desviopadrão
d <-47
#Médiadaamostra
x <-658
#Tamanhodaamostra
n <-100
#Nível de confiança
nc<-(1-0.90)/2
#Erro
error <-d/sqrt(n)
#Limiteinferior
left <-x-(qnorm(nc,lower.tail= F)*error)
#Limitesuperior
right <-x+(qnorm(nc,lower.tail= F)*error)
cat("[",left, "-", right,"]")
cat("[",left, "-", right,"]", sep = "")
aux <- c(4.37, 3.63, 2.78, 5.46, 2.18, 6.07, 3.24, 5.89, 4.86, 4.64)
sd(aux)
length(aux)
aux <- c(4.37, 3.63, 2.78, 5.46, 2.18, 6.07, 3.24, 5.89, 4.86, 4.64)
#Desviopadrão
d <-sd(aux)
#Médiadaamostra
x <-mean(aux)
#Tamanhodaamostra
n <-length(aux)
#Nível de confiança
nc<-(1-0.90)/2
#Erro
error <-d/sqrt(n)
#Limiteinferior
left <-x-(qnorm(nc,lower.tail= F)*error)
#Limitesuperior
right <-x+(qnorm(nc,lower.tail= F)*error)
cat("[",left, "-", right,"]", sep = "")
cat("[",left, "-", right,"]", sep = "")
library(tm)
library(wordcloud)
library(syuzhet)
library(stringr)
library(stringi)
library(readr)
library(dplyr)
setwd("~/GitHub/SUS-Sentiment-Analysis/Projeto")
sent_labels <- c("Raiva", "Antecipação", "Nojo", "Medo", "Alegria",
"Tristeza", "Surpesa", "Confiança", "Negativo", "Positivo", "Neutral")
months <- c("Dez 2019", "Jan 2020", "Fev 2020", "Mar 2020", "Abril 2020",
"Maio 2020")
textclean1 <- function(text_list){
aux <- text_list
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux <-  readr::parse_character(aux, locale = readr::locale('pt'))
aux <- sapply(aux, function(x) stri_trans_tolower(x,'pt')) # Por tudo em letras minúsculas
aux <- gsub("<[Uu]\\+[a-zA-Z0-9]*>", "",  aux); # Remover Emojis
aux <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "",  aux)
aux <- str_replace(aux,"RT @[a-z,A-Z]*: ","")
aux <- gsub("@\\w+", "", aux)
aux <- removeURL(aux)
aux <- str_replace_all(aux,"@[a-zA-Z]*","") # Remover marcações @username
aux <- gsub("[^[:alnum:][:blank:]!?]", "", aux)
aux <- gsub("[[:digit:]]", "", aux)
return(aux)
}
build_corpus <- function(text_listf, rwords=NULL){
tweets_t <- paste(text_listf,collapse=" ")
tweets_S <- VectorSource(tweets_t)
corpus <- Corpus(tweets_S)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
corpus <- tm_map(corpus, removeWords, rwords)
return(corpus)
}
get_sentiment <- function(text_listf, title="Sentimentos"){
s <- get_nrc_sentiment(text_listf, language = "portuguese")
s$neutral <- (s$positive == s$negative) > 0
s$positive <- (s$positive > s$negative) > 0
s$negative <- (s$positive == 0 & s$neutral == 0) > 0
colnames(s) <- sent_labels
barplot(colSums(s),las=2,col=rainbow(10), ylab= "Quantidade", main=title)
return(s)
}
get_freq <- function(text_listf, rwords=NULL, cloud_max_words = 100){
corpus <- build_corpus(text_listf, rwords)
dtm <-TermDocumentMatrix(corpus)
dtm <- as.matrix(dtm)
fre <- sort(rowSums(dtm),decreasing=TRUE)
wordcloud(names(fre),freq=fre,min.freq=1,max.words=cloud_max_words,scale=c(4,.5),
random.order=F, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
return(fre)
}
tweets <- read.csv("new_tweetsdb.csv", encoding = "UTF-8")
tweets$date <- str_extract(tweets$date, "[0-9]{4}-[0-9]{2}")
tweets$textf <- textclean1(tweets$tweet)
tp1 <- tweets[tweets$date == "2019-12",][1000,]
tp2 <- tweets[tweets$date == "2020-01",][1000,]
tp3 <- tweets[tweets$date == "2020-02",][1000,]
tp4 <- tweets[tweets$date == "2020-03",][1000,]
tp5 <- tweets[tweets$date == "2020-04",][1000,]
tp6 <- tweets[tweets$date == "2020-05",][1000,]
View(tp1)
View(tweets)
View(f)
tp1 <- tweets[tweets$date == "2019-12",][1000,]
tp2 <- tweets[tweets$date == "2020-01",][1000,]
tp3 <- tweets[tweets$date == "2020-02",][1000,]
tp4 <- tweets[tweets$date == "2020-03",][1000,]
tp5 <- tweets[tweets$date == "2020-04",][1000,]
tp6 <- tweets[tweets$date == "2020-05",][1000,]
tweets$date == "2019-12"
tweets$date == "2020-05"
tweets <- read.csv("new_tweetsdb.csv", encoding = "UTF-8")
aux <- tweets$tweet
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux
aux[1]
tweets <- read.csv("new_tweetsdb.csv", encoding = "UTF-8")
aux <- tweets$tweet
aux[1]
tweets <- read.csv("new_tweetsdb.csv", encoding = "Unicode")
aux <- tweets$tweet
tweets$textf <- textclean1(tweets$tweet)
setwd("~/GitHub/SUS-Sentiment-Analysis/Projeto/SUS_Saude")
a <- read.csv(file = "tweets1.csv", encoding = "UTF-8")
View(a)
b <- read.csv(file = "tweets2.csv", encoding = "UTF-8")
c <- read.csv(file = "tweets3.csv", encoding = "UTF-8")
d <- read.csv(file = "tweets4.csv", encoding = "UTF-8")
e <- read.csv(file = "tweets5.csv", encoding = "UTF-8")
f <- read.csv(file = "tweets6.csv", encoding = "UTF-8")
g <- read.csv(file = "tweets7.csv", encoding = "UTF-8")
h <- read.csv(file = "tweets8.csv", encoding = "UTF-8")
i <- read.csv(file = "tweets9.csv", encoding = "UTF-8")
j <- read.csv(file = "tweets10.csv", encoding = "UTF-8")
db <- rbind(a, b, c, d, e, f, g, h, i, j)
View(db)
write.csv(x = db, file = "../new_tweetsdb.csv")
library(tm)
library(wordcloud)
library(syuzhet)
library(stringr)
library(stringi)
library(readr)
library(dplyr)
setwd("~/GitHub/SUS-Sentiment-Analysis/Projeto")
sent_labels <- c("Raiva", "Antecipação", "Nojo", "Medo", "Alegria",
"Tristeza", "Surpesa", "Confiança", "Negativo", "Positivo", "Neutral")
months <- c("Dez 2019", "Jan 2020", "Fev 2020", "Mar 2020", "Abril 2020",
"Maio 2020")
textclean1 <- function(text_list){
aux <- text_list
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux <-  readr::parse_character(aux, locale = readr::locale('pt'))
aux <- sapply(aux, function(x) stri_trans_tolower(x,'pt')) # Por tudo em letras minúsculas
aux <- gsub("<[Uu]\\+[a-zA-Z0-9]*>", "",  aux); # Remover Emojis
aux <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "",  aux)
aux <- str_replace(aux,"RT @[a-z,A-Z]*: ","")
aux <- gsub("@\\w+", "", aux)
aux <- removeURL(aux)
aux <- str_replace_all(aux,"@[a-zA-Z]*","") # Remover marcações @username
aux <- gsub("[^[:alnum:][:blank:]!?]", "", aux)
aux <- gsub("[[:digit:]]", "", aux)
return(aux)
}
build_corpus <- function(text_listf, rwords=NULL){
tweets_t <- paste(text_listf,collapse=" ")
tweets_S <- VectorSource(tweets_t)
corpus <- Corpus(tweets_S)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
corpus <- tm_map(corpus, removeWords, rwords)
return(corpus)
}
get_sentiment <- function(text_listf, title="Sentimentos"){
s <- get_nrc_sentiment(text_listf, language = "portuguese")
s$neutral <- (s$positive == s$negative) > 0
s$positive <- (s$positive > s$negative) > 0
s$negative <- (s$positive == 0 & s$neutral == 0) > 0
colnames(s) <- sent_labels
barplot(colSums(s),las=2,col=rainbow(10), ylab= "Quantidade", main=title)
return(s)
}
get_freq <- function(text_listf, rwords=NULL, cloud_max_words = 100){
corpus <- build_corpus(text_listf, rwords)
dtm <-TermDocumentMatrix(corpus)
dtm <- as.matrix(dtm)
fre <- sort(rowSums(dtm),decreasing=TRUE)
wordcloud(names(fre),freq=fre,min.freq=1,max.words=cloud_max_words,scale=c(4,.5),
random.order=F, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
return(fre)
}
tweets <- read.csv("new_tweetsdb.csv", encoding = "Unicode")
aux <- tweets$tweet
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux <-  readr::parse_character(aux, locale = readr::locale('pt'))
aux <- sapply(aux, function(x) stri_trans_tolower(x,'pt')) # Por tudo em letras minúsculas
aux <- gsub("<[Uu]\\+[a-zA-Z0-9]*>", "",  aux); # Remover Emojis
aux <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "",  aux)
aux <- str_replace(aux,"RT @[a-z,A-Z]*: ","")
aux <- gsub("@\\w+", "", aux)
aux <- removeURL(aux)
aux <- str_replace_all(aux,"@[a-zA-Z]*","") # Remover marcações @username
aux <- gsub("[^[:alnum:][:blank:]!?]", "", aux)
aux <- gsub("[[:digit:]]", "", aux)
aux[1]
tweets$date <- str_extract(tweets$date, "[0-9]{4}-[0-9]{2}")
tweets$textf <- textclean1(tweets$tweet)
library(tm)
library(wordcloud)
library(syuzhet)
library(stringr)
library(stringi)
library(readr)
library(dplyr)
setwd("~/GitHub/SUS-Sentiment-Analysis/Projeto")
sent_labels <- c("Raiva", "Antecipação", "Nojo", "Medo", "Alegria",
"Tristeza", "Surpesa", "Confiança", "Negativo", "Positivo", "Neutral")
months <- c("Dez 2019", "Jan 2020", "Fev 2020", "Mar 2020", "Abril 2020",
"Maio 2020")
textclean1 <- function(text_list){
aux <- text_list
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux <-  readr::parse_character(aux, locale = readr::locale('pt'))
aux <- sapply(aux, function(x) stri_trans_tolower(x,'pt')) # Por tudo em letras minúsculas
aux <- gsub("<[Uu]\\+[a-zA-Z0-9]*>", "",  aux); # Remover Emojis
aux <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "",  aux)
aux <- str_replace(aux,"RT @[a-z,A-Z]*: ","")
aux <- gsub("@\\w+", "", aux)
aux <- removeURL(aux)
aux <- str_replace_all(aux,"@[a-zA-Z]*","") # Remover marcações @username
aux <- gsub("[^[:alnum:][:blank:]!?]", "", aux)
aux <- gsub("[[:digit:]]", "", aux)
return(aux)
}
build_corpus <- function(text_listf, rwords=NULL){
tweets_t <- paste(text_listf,collapse=" ")
tweets_S <- VectorSource(tweets_t)
corpus <- Corpus(tweets_S)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
corpus <- tm_map(corpus, removeWords, rwords)
return(corpus)
}
get_sentiment <- function(text_listf, title="Sentimentos"){
s <- get_nrc_sentiment(text_listf, language = "portuguese")
s$neutral <- (s$positive == s$negative) > 0
s$positive <- (s$positive > s$negative) > 0
s$negative <- (s$positive == 0 & s$neutral == 0) > 0
colnames(s) <- sent_labels
barplot(colSums(s),las=2,col=rainbow(10), ylab= "Quantidade", main=title)
return(s)
}
get_freq <- function(text_listf, rwords=NULL, cloud_max_words = 100){
corpus <- build_corpus(text_listf, rwords)
dtm <-TermDocumentMatrix(corpus)
dtm <- as.matrix(dtm)
fre <- sort(rowSums(dtm),decreasing=TRUE)
wordcloud(names(fre),freq=fre,min.freq=1,max.words=cloud_max_words,scale=c(4,.5),
random.order=F, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
return(fre)
}
textclean1 <- function(text_list){
aux <- text_list
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux <-  readr::parse_character(aux, locale = readr::locale('pt'))
aux <- sapply(aux, function(x) stri_trans_tolower(x,'pt')) # Por tudo em letras minúsculas
aux <- gsub("<[Uu]\\+[a-zA-Z0-9]*>", "",  aux); # Remover Emojis
aux <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "",  aux)
aux <- str_replace(aux,"RT @[a-z,A-Z]*: ","")
aux <- gsub("@\\w+", "", aux)
aux <- removeURL(aux)
aux <- str_replace_all(aux,"@[a-zA-Z]*","") # Remover marcações @username
aux <- gsub("[^[:alnum:][:blank:]!?]", "", aux)
aux <- gsub("[[:digit:]]", "", aux)
return(aux)
}
build_corpus <- function(text_listf, rwords=NULL){
tweets_t <- paste(text_listf,collapse=" ")
tweets_S <- VectorSource(tweets_t)
corpus <- Corpus(tweets_S)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
corpus <- tm_map(corpus, removeWords, rwords)
return(corpus)
}
get_sentiment <- function(text_listf, title="Sentimentos"){
s <- get_nrc_sentiment(text_listf, language = "portuguese")
s$neutral <- (s$positive == s$negative) > 0
s$positive <- (s$positive > s$negative) > 0
s$negative <- (s$positive == 0 & s$neutral == 0) > 0
colnames(s) <- sent_labels
barplot(colSums(s),las=2,col=rainbow(10), ylab= "Quantidade", main=title)
return(s)
}
get_freq <- function(text_listf, rwords=NULL, cloud_max_words = 100){
corpus <- build_corpus(text_listf, rwords)
dtm <-TermDocumentMatrix(corpus)
dtm <- as.matrix(dtm)
fre <- sort(rowSums(dtm),decreasing=TRUE)
wordcloud(names(fre),freq=fre,min.freq=1,max.words=cloud_max_words,scale=c(4,.5),
random.order=F, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
return(fre)
}
tweets <- read.csv("new_tweetsdb.csv", encoding = "Unicode")
tweets$date <- str_extract(tweets$date, "[0-9]{4}-[0-9]{2}")
tweets$textf <- textclean1(tweets$tweet)
tweets <- read.csv("new_tweetsdb.csv", encoding = "UTF-8")
tweets$date <- str_extract(tweets$date, "[0-9]{4}-[0-9]{2}")
tweets$textf <- textclean1(tweets$tweet)
View(tweets)
write.csv(x = db, file = "../new_tweetsdb.csv", encoding = "UTF-8")
setwd("~/GitHub/SUS-Sentiment-Analysis/Projeto/SUS_Saude")
a <- read.csv(file = "tweets1.csv", encoding = "UTF-8")
b <- read.csv(file = "tweets2.csv", encoding = "UTF-8")
c <- read.csv(file = "tweets3.csv", encoding = "UTF-8")
d <- read.csv(file = "tweets4.csv", encoding = "UTF-8")
e <- read.csv(file = "tweets5.csv", encoding = "UTF-8")
f <- read.csv(file = "tweets6.csv", encoding = "UTF-8")
g <- read.csv(file = "tweets7.csv", encoding = "UTF-8")
h <- read.csv(file = "tweets8.csv", encoding = "UTF-8")
i <- read.csv(file = "tweets9.csv", encoding = "UTF-8")
j <- read.csv(file = "tweets10.csv", encoding = "UTF-8")
db <- rbind(a, b, c, d, e, f, g, h, i, j)
write.csv(x = db, file = "../new_tweetsdb.csv", fileEncoding = "UTF-8")
library(tm)
library(wordcloud)
library(syuzhet)
library(stringr)
library(stringi)
library(readr)
library(dplyr)
setwd("~/GitHub/SUS-Sentiment-Analysis/Projeto")
sent_labels <- c("Raiva", "Antecipação", "Nojo", "Medo", "Alegria",
"Tristeza", "Surpesa", "Confiança", "Negativo", "Positivo", "Neutral")
months <- c("Dez 2019", "Jan 2020", "Fev 2020", "Mar 2020", "Abril 2020",
"Maio 2020")
textclean1 <- function(text_list){
aux <- text_list
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
aux <-  readr::parse_character(aux, locale = readr::locale('pt'))
aux <- sapply(aux, function(x) stri_trans_tolower(x,'pt')) # Por tudo em letras minúsculas
aux <- gsub("<[Uu]\\+[a-zA-Z0-9]*>", "",  aux); # Remover Emojis
aux <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "",  aux)
aux <- str_replace(aux,"RT @[a-z,A-Z]*: ","")
aux <- gsub("@\\w+", "", aux)
aux <- removeURL(aux)
aux <- str_replace_all(aux,"@[a-zA-Z]*","") # Remover marcações @username
aux <- gsub("[^[:alnum:][:blank:]!?]", "", aux)
aux <- gsub("[[:digit:]]", "", aux)
return(aux)
}
build_corpus <- function(text_listf, rwords=NULL){
tweets_t <- paste(text_listf,collapse=" ")
tweets_S <- VectorSource(tweets_t)
corpus <- Corpus(tweets_S)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
corpus <- tm_map(corpus, removeWords, rwords)
return(corpus)
}
get_sentiment <- function(text_listf, title="Sentimentos"){
s <- get_nrc_sentiment(text_listf, language = "portuguese")
s$neutral <- (s$positive == s$negative) > 0
s$positive <- (s$positive > s$negative) > 0
s$negative <- (s$positive == 0 & s$neutral == 0) > 0
colnames(s) <- sent_labels
barplot(colSums(s),las=2,col=rainbow(10), ylab= "Quantidade", main=title)
return(s)
}
get_freq <- function(text_listf, rwords=NULL, cloud_max_words = 100){
corpus <- build_corpus(text_listf, rwords)
dtm <-TermDocumentMatrix(corpus)
dtm <- as.matrix(dtm)
fre <- sort(rowSums(dtm),decreasing=TRUE)
wordcloud(names(fre),freq=fre,min.freq=1,max.words=cloud_max_words,scale=c(4,.5),
random.order=F, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
return(fre)
}
tweets <- read.csv("new_tweetsdb.csv", encoding = "UTF-8")
tweets$date <- str_extract(tweets$date, "[0-9]{4}-[0-9]{2}")
tweets$textf <- textclean1(tweets$tweet)
View(tweets)
tp1 <- tweets[tweets$date == "2019-12",][1:1000,]
tp2 <- tweets[tweets$date == "2020-01",][1:1000,]
tp3 <- tweets[tweets$date == "2020-02",][1:1000,]
tp4 <- tweets[tweets$date == "2020-03",][1:1000,]
tp5 <- tweets[tweets$date == "2020-04",][1:1000,]
tp6 <- tweets[tweets$date == "2020-05",][1:1000,]
ncloud = 200
words<- c("sus", "pra", "pro", "né", "tá", "vcs", "tô", "aí")
tp1.fre <- get_freq(tp1$textf,  rwords=words, cloud_max_words = ncloud)
tp1.s <- get_sentiment(tp1$textf, "Sentimentos Dez 2019")
tp2.fre <- get_freq(tp2$textf,  rwords=words, cloud_max_words = ncloud)
tp2.s <- get_sentiment(tp2$textf, "Sentimentos Jan 2020")
tp3.fre <- get_freq(tp3$textf,  rwords=words, cloud_max_words = ncloud)
tp3.s <- get_sentiment(tp3$textf, "Sentimentos Fev 2020")
tp4.fre <- get_freq(tp4$textf,  rwords=words, cloud_max_words = ncloud)
tp4.s <- get_sentiment(tp4$textf, "Sentimentos Mar 2020")
tp5.fre <- get_freq(tp5$textf,  rwords=words, cloud_max_words = ncloud)
tp5.s <- get_sentiment(tp5$textf, "Sentimentos Abril 2020")
tp6.fre <- get_freq(tp6$textf,  rwords=words, cloud_max_words = ncloud)
tp6.s <- get_sentiment(tp6$textf, "Sentimentos Maio 2020")
#df <- matrix(c(colSums(tp1.s),colSums(tp2.s),colSums(tp3.s),colSums(tp4.s),colSums(tp5.s),colSums(tp6.s)), nrow = 6, ncol = 10, byrow = T)
#barplot(df, col=rainbow(length(pos_neg[1,])), names.arg = sent_labels, beside = T, las=2)
#legend("topleft", pch = 15, col = rainbow(length(pos_neg[1,])), months)
df <- matrix(c(colSums(tp1.s>0),colSums(tp2.s>0),colSums(tp3.s>0),colSums(tp4.s>0),colSums(tp5.s>0),colSums(tp6.s>0)), nrow = 11, ncol = 6, byrow = F)
barplot(df, col=rainbow(length(df[,1])), names.arg = months, beside = T)
legend("topleft", pch = 15, col = rainbow(length(df[,1])), sent_labels, cex = 0.75)
col_posneg <- c(9:11)
pos_neg <- df[col_posneg,]
aux <- pos_neg
for (i in c(1:length(pos_neg[1,]))) {
for (j in c(1:length(pos_neg[,1]))) {
pos_neg[j,i] <- (pos_neg[j,i]/ sum(aux[,i]))*100
}
}
col_sents <- c(1, 3, 4, 5, 6, 8)
sent <- df[c(col_sents),]
barplot(pos_neg, col=rainbow(length(pos_neg[,1])), names.arg = months, beside = T, xlab = "Mês", ylab="Percentual(%)", main= "Sentimentos positivos e negativos no período de pesquisa")
legend("topright", pch = 15, col = rainbow(length(pos_neg[,1])), sent_labels[col_posneg], cex = 0.75)
